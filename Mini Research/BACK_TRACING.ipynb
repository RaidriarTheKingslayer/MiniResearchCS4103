{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b3d0a9-98f2-42f3-ab89-b105620d74b2",
   "metadata": {},
   "source": [
    "Back Tracing is the processing of reverse engineering a function back to the original formula. Most researchers do this to understand which part of the mathematical foundations of the algorithmic function can be further improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e57f88-ec69-4c5c-a4b2-b3c4c2c6e310",
   "metadata": {},
   "source": [
    "Example, you are given a code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5627ca9-fc7a-49ed-8ac7-31f1be9f647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1,x2,y1,y2):\n",
    "    z = ((x1-x2)**2+(y1-y2)**2)**(0.5)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290847bf-c601-4307-90de-85d3de449229",
   "metadata": {},
   "source": [
    "The formula above is a function for the Euclidean distance, back tracing (reverse engineering) it will yield:\n",
    "\n",
    "$$\n",
    "z = \\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin Actual Backtrace ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b587",
   "metadata": {},
   "source": [
    "I shall be backtracing the functions Comprising the Multinomial Expectation Maximizer that utilizes essential mathematical formulas for a Multinomial Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4946a2f",
   "metadata": {},
   "source": [
    "First off , The MultinomialExpectationMaximizer object is initialized using the following code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialExpectationMaximizer(k, restarts=10, rtol=1e-3)\n",
    "\n",
    "def __init__(self, K, rtol=1e-3, max_iter=100, restarts=10):\n",
    "        self._K = K\n",
    "        self._rtol = rtol\n",
    "        self._max_iter = max_iter\n",
    "        self._restarts = restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d77a",
   "metadata": {},
   "source": [
    "The parameters are as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87c958",
   "metadata": {},
   "source": [
    "k: represents the number of mixture components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4a609",
   "metadata": {},
   "source": [
    "rtol=1e-3 : Sets the relative tolerance for convergence that will stop the algorithm when the change in log-likelihood between iterations is less than this value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2dfb9f",
   "metadata": {},
   "source": [
    "max_iter=100 : Sets the maximum number of iterations for the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796a9bb",
   "metadata": {},
   "source": [
    "restarts=10 : Sets the number of times the EM algorithm will be restarted with random initializations to avoid getting stuck in local optima and finding a better overall solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a656f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_once(self, X):\n",
    "        '''\n",
    "        Runs one full cycle of the EM algorithm\n",
    "\n",
    "        :param X: (N, C), matrix of counts\n",
    "        :return: The best parameters found along with the associated loss\n",
    "        '''\n",
    "        loss = float('inf')\n",
    "        alpha, beta = self._init_params(X)\n",
    "        gamma = None\n",
    "\n",
    "        for it in range(self._max_iter):\n",
    "            prev_loss = loss\n",
    "            gamma = self._e_step(X, alpha, beta)\n",
    "            alpha, beta = self._m_step(X, gamma)\n",
    "            loss = self._compute_vlb(X, alpha, beta, gamma)\n",
    "            print('Loss: %f' % loss)\n",
    "            if it > 0 and np.abs((prev_loss - loss) / prev_loss) < self._rtol:\n",
    "                    break\n",
    "        return alpha, beta, gamma, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649241f6",
   "metadata": {},
   "source": [
    "def _train_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cd726",
   "metadata": {},
   "source": [
    "the function runs a single iteration of the EM algorithm for training the model and returns the best parameters alpha and beta, the latent variables gamma, and the associated loss. The breakdown of the function is as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b4c6c",
   "metadata": {},
   "source": [
    "- It initializes the loss to infinity and calls the _init_params method to initialize the parameters alpha and beta based on the input X.\n",
    "- It sets the latent variables gamma to None.\n",
    "- It iterates over a maximum number of iterations _max_iter\n",
    "- In each iteration, it does the following:\n",
    "  1. It saves the previous loss value.\n",
    "  2. It calls the _e_step method to perform the expectation step of the EM algorithm\n",
    "  3. It calls the _m_step method to perform the maximization step of the EM algorithm\n",
    "  4. It calls the _compute_vlb method to compute the variational lower bound (VLB) of the log-likelihood\n",
    "  5. It prints the current loss value.\n",
    "  6. It checks if the relative change in the loss value is smaller than a tolerance _rtol, and If so, it breaks the loop and stops the iteration.\n",
    "- It returns the final values of alpha, beta, gamma, and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477ac6e",
   "metadata": {},
   "source": [
    "Here are the following formulas used in this function :\n",
    "\n",
    "Weighted multinomial (posterior) probability for object i and cluster k (weighted_multi_prob[i, k] / P(k|X_i)):\n",
    "$$P(k|X_i) ∝ P(X_i|k) * P(k)$$\n",
    "\n",
    "Denominator for normalization of posterior probabilities for object i (denum) :\n",
    "$$denum_i = sum(P(k|X_i) for k in K)$$\n",
    "\n",
    "Posterior probability of object i belonging to cluster k (gamma/P(k|X_i)):\n",
    "$$P(k|X_i) = P(X_i|k) * P(k) / sum(P(j|X_i) for j in K)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _m_step(self, X, gamma):\n",
    "        \"\"\"\n",
    "        Performs M-step on MNMM model\n",
    "        Each input is numpy array:\n",
    "        X: (N x C), matrix of counts\n",
    "        gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "\n",
    "        Returns:\n",
    "        alpha: (K), mixture component weights\n",
    "        beta: (K x C), mixture categories weights\n",
    "        \"\"\"\n",
    "        # Compute alpha\n",
    "        alpha = self._m_step_alpha(gamma)\n",
    "\n",
    "        # Compute beta\n",
    "        beta = self._m_step_beta(X, gamma)\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "    def _m_step_alpha(self, gamma):\n",
    "        alpha = gamma.sum(axis=0) / gamma.sum()\n",
    "        return alpha\n",
    "\n",
    "    def _m_step_beta(self, X, gamma):\n",
    "        weighted_counts = gamma.T.dot(X)\n",
    "        beta = weighted_counts / weighted_counts.sum(axis=-1).reshape(-1, 1)\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62757d",
   "metadata": {},
   "source": [
    "def _m_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fae57",
   "metadata": {},
   "source": [
    "This code implements the M-step of the model  to update the model parameters (alpha and beta) based on the posterior probabilities (gamma) estimated in the E-step by calling m_step_alpha and m_step_beta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe72d42",
   "metadata": {},
   "source": [
    "Updated mixture weights (alpha):\n",
    "$$α_k = ∑_i^N γ_ik / ∑_i^N ∑_k^K γ_ik$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f333b42",
   "metadata": {},
   "source": [
    "Updated multinomial weights (beta): \n",
    "$$β_kc = ∑_i^N γ_ik * x_ic / ∑_j^C ∑_i^N γ_ik * x_ij$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_vlb(self, X, alpha, beta, gamma):\n",
    "        \"\"\"\n",
    "        Computes the variational lower bound\n",
    "        X: (N x C), data points\n",
    "        alpha: (K) or (NxK) with individual weights, mixture component weights\n",
    "        beta: (K x C), multinomial categories weights\n",
    "        gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "\n",
    "        Returns value of variational lower bound\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for k in range(beta.shape[0]):\n",
    "            weights = gamma[:, k]\n",
    "            loss += np.sum(weights * (np.log(self._get_mixture_weight(alpha, k)) + self._multinomial_prob(X, beta[k], log=True)))\n",
    "            loss -= np.sum(weights * np.log(weights))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f14f00",
   "metadata": {},
   "source": [
    "def _compute_vlb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afd766",
   "metadata": {},
   "source": [
    "This code calculates the VLB (loss) for the model that will be used in variational inference to approximate model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10d02c",
   "metadata": {},
   "source": [
    "$$VLB = ∑_k ∑_i γ_ik * (log(α_k) + log(p(X_i|k))) - ∑_k ∑_i γ_ik * log(γ_ik)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multinomial_prob(self, counts, beta, log=False):\n",
    "        \"\"\"\n",
    "        Evaluates the multinomial probability for a given vector of counts\n",
    "        counts: (N x C), matrix of counts\n",
    "        beta: (C), vector of multinomial parameters for a specific cluster k\n",
    "        Returns:\n",
    "        p: (N), scalar values for the probabilities of observing each count vector given the beta parameters\n",
    "        \"\"\"\n",
    "        n = counts.sum(axis=-1)\n",
    "        m = multinomial(n, beta)\n",
    "        if log:\n",
    "            return m.logpmf(counts)\n",
    "        return m.pmf(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda8ee5",
   "metadata": {},
   "source": [
    "def _multinomial_prob\n",
    "\n",
    "This code computes the multinomial probability of observing a given vector of counts with specified parameters (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f8cdc",
   "metadata": {},
   "source": [
    "$$P(X|k) = (n! / (x1! * x2! * ... * xC!)) * (beta_k1^x1 * beta_k2^x2 * ... * beta_kC^xC)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(self, X_test, alpha, beta):\n",
    "        mn_probs = np.zeros(X_test.shape[0])\n",
    "        for k in range(beta.shape[0]):\n",
    "            mn_probs_k = self._get_mixture_weight(alpha, k) * self._multinomial_prob(X_test, beta[k])\n",
    "            mn_probs += mn_probs_k\n",
    "        mn_probs[mn_probs == 0] = np.finfo(float).eps\n",
    "        return np.log(mn_probs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a342ce",
   "metadata": {},
   "source": [
    "def compute_log_likelihood\n",
    "\n",
    "the function calculates the log-likelihood (Log(L)) of a given dataset (X_test) with specified parameters (alpha and beta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b824ee",
   "metadata": {},
   "source": [
    "$$log(L) = ∑_i log(∑_k α_k * p(X_i|k))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60687cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bic(self, X_test, alpha, beta, log_likelihood=None):\n",
    "        if log_likelihood is None:\n",
    "            log_likelihood = self.compute_log_likelihood(X_test, alpha, beta)\n",
    "        N = X_test.shape[0]\n",
    "        return np.log(N) * (alpha.size + beta.size) - 2 * log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65f95d",
   "metadata": {},
   "source": [
    "def compute_bic\n",
    "\n",
    " Calculates the BIC given model parameters (alpha and beta) and a dataset (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31049156",
   "metadata": {},
   "source": [
    "$$BIC = log(N) * d - 2 * log(L)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea67a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_icl_bic(self, bic, gamma):\n",
    "        classification_entropy = -(np.log(gamma.max(axis=1))).sum()\n",
    "        return bic - classification_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9f8b7",
   "metadata": {},
   "source": [
    "def compute_icl_bic \n",
    "\n",
    "calculates the ICL-BIC, a variant of the BIC that incorporates a penalty for model complexity based on cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850da2a8",
   "metadata": {},
   "source": [
    "$$ICL = BIC - ∑_i log(max_k γ_ik)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
